{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Github :\n",
        "Linkedin :\n",
        "Medium :\n"
      ],
      "metadata": {
        "id": "I7nstCyeM7tK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis and Preprocessing of Car Dataset\n",
        "\n",
        "In this notebook, we will perform data preprocessing on a dataset containing information about various cars. The dataset includes attributes such as miles per gallon (MPG), the number of cylinders, and other characteristics that may influence fuel efficiency. Our goal is to prepare the data for further analysis or machine learning modeling.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Import Libraries](#import-libraries)\n",
        "2. [Load the Dataset](#load-the-dataset)\n",
        "3. [Explore the Initial Data](#explore-the-initial-data)\n",
        "4. [Rename Columns](#rename-columns)\n",
        "5. [Handle Missing Values](#handle-missing-values)\n",
        "6. [Drop Unnecessary Columns](#drop-unnecessary-columns)\n",
        "7. [One-Hot Encoding of Categorical Variables](#one-hot-encoding-of-categorical-variables)\n",
        "8. [Separate Features and Target Variable](#separate-features-and-target-variable)\n",
        "9. [Train-Test Split](#train-test-split)\n",
        "10. [Feature Scaling](#feature-scaling)\n",
        "11. [Display Preprocessed Data](#display-preprocessed-data)\n",
        "\n",
        "## Step 1: Import Libraries <a name=\"import-libraries\"></a>\n",
        "\n",
        "To begin, we will import the necessary libraries. We need `pandas` for data manipulation and `sklearn` for data preprocessing and model evaluation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xnfrR2kSLReL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3fWAzXKwLInU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Load the Dataset <a name=\"load-the-dataset\"></a>**\n",
        "Next, we will load the dataset from the provided URL. The dataset is in CSV format and can be read into a pandas DataFrame."
      ],
      "metadata": {
        "id": "oKZyN41YLgzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://archive.ics.uci.edu/static/public/9/data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "print(\"Initial DataFrame:\\n\", df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtIpZZWPLV-J",
        "outputId": "9f42b5a2-ea25-48d6-8ee9-7ad0bd364baf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial DataFrame:\n",
            "                     car_name  cylinders  displacement  horsepower  weight  \\\n",
            "0  chevrolet,chevelle,malibu          8         307.0       130.0    3504   \n",
            "1          buick,skylark,320          8         350.0       165.0    3693   \n",
            "2         plymouth,satellite          8         318.0       150.0    3436   \n",
            "3              amc,rebel,sst          8         304.0       150.0    3433   \n",
            "4                ford,torino          8         302.0       140.0    3449   \n",
            "\n",
            "   acceleration  model_year  origin   mpg  \n",
            "0          12.0          70       1  18.0  \n",
            "1          11.5          70       1  15.0  \n",
            "2          11.0          70       1  18.0  \n",
            "3          12.0          70       1  16.0  \n",
            "4          10.5          70       1  17.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "We use pd.read_csv() to read the dataset from the URL.\n",
        "The head() method displays the first few rows of the DataFrame, giving us a glimpse of the data."
      ],
      "metadata": {
        "id": "bTCij5x2LnpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Explore the Initial Data <a name=\"explore-the-initial-data\"></a>**\n",
        "Before preprocessing, it's helpful to understand the structure of the dataset. This can include checking for data types, missing values, and overall dimensions."
      ],
      "metadata": {
        "id": "ge5oa5WYLq9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DataFrame Shape:\", df.shape)\n",
        "print(\"DataFrame Info:\")\n",
        "df.info()\n",
        "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gj_462LLlqS",
        "outputId": "aaf46f3f-7aff-461a-a0a3-681e4ae5700f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame Shape: (398, 9)\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 398 entries, 0 to 397\n",
            "Data columns (total 9 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   car_name      398 non-null    object \n",
            " 1   cylinders     398 non-null    int64  \n",
            " 2   displacement  398 non-null    float64\n",
            " 3   horsepower    392 non-null    float64\n",
            " 4   weight        398 non-null    int64  \n",
            " 5   acceleration  398 non-null    float64\n",
            " 6   model_year    398 non-null    int64  \n",
            " 7   origin        398 non-null    int64  \n",
            " 8   mpg           398 non-null    float64\n",
            "dtypes: float64(4), int64(4), object(1)\n",
            "memory usage: 28.1+ KB\n",
            "\n",
            "Missing Values:\n",
            " car_name        0\n",
            "cylinders       0\n",
            "displacement    0\n",
            "horsepower      6\n",
            "weight          0\n",
            "acceleration    0\n",
            "model_year      0\n",
            "origin          0\n",
            "mpg             0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "df.shape provides the number of rows and columns in the DataFrame.\n",
        "df.info() displays the data types and the number of non-null values in each column.\n",
        "df.isnull().sum() shows the count of missing values in each column."
      ],
      "metadata": {
        "id": "vP-Bf2grLvAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Rename Columns <a name=\"rename-columns\"></a>**\n",
        "For easier access and readability, we will rename the columns of the DataFrame."
      ],
      "metadata": {
        "id": "dhXcGGRzLxMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming the columns for easier access\n",
        "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'car_name']\n"
      ],
      "metadata": {
        "id": "WLcyrW8PLtNU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "Clear column names help avoid confusion and make our code more readable."
      ],
      "metadata": {
        "id": "SLxeDALUL24p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Handle Missing Values <a name=\"handle-missing-values\"></a>**\n",
        "Next, we will address the missing values, particularly in the horsepower column. We will convert the column to numeric values, coercing any errors, and fill missing values with the mean of the column."
      ],
      "metadata": {
        "id": "DCuHyeV4L5ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values (replace missing horsepower values with the mean)\n",
        "df['horsepower'] = pd.to_numeric(df['horsepower'], errors='coerce')\n",
        "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].mean())\n"
      ],
      "metadata": {
        "id": "qs9bQJ_wL1aC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "pd.to_numeric() converts the horsepower column to numeric, replacing non-numeric values with NaN.\n",
        "We then fill missing values with the mean of the horsepower column using fillna()."
      ],
      "metadata": {
        "id": "9NcivEGPL9ZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6: Drop Unnecessary Columns <a name=\"drop-unnecessary-columns\"></a>**\n",
        "We will drop the car_name column, as it does not provide useful information for our analysis."
      ],
      "metadata": {
        "id": "nS10G6YZMCD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop 'car_name' as it's not useful in our analysis\n",
        "df.drop('car_name', axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "vt4CsT3kL7n8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "The drop() method removes the specified column (car_name) from the DataFrame."
      ],
      "metadata": {
        "id": "9hK2cWgRMHdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 7: One-Hot Encoding of Categorical Variables <a name=\"one-hot-encoding-of-categorical-variables\"></a>**\n",
        "To convert the categorical origin column into a format suitable for modeling, we will apply one-hot encoding."
      ],
      "metadata": {
        "id": "wN7ly5pbMNgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode 'origin' column\n",
        "df = pd.get_dummies(df, columns=['origin'], prefix='origin')\n"
      ],
      "metadata": {
        "id": "2ODVwd4JMFmN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "pd.get_dummies() converts categorical variables into a series of binary columns (0s and 1s), allowing us to use these features in a regression model."
      ],
      "metadata": {
        "id": "y3DLnq8oMShy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 8: Separate Features and Target Variable <a name=\"separate-features-and-target-variable\"></a>**\n",
        "We will now separate the features (independent variables) from the target variable (dependent variable, which is MPG in this case)."
      ],
      "metadata": {
        "id": "u_vpQ5EDMWAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target\n",
        "X = df.drop('mpg', axis=1)\n",
        "y = df['mpg']\n"
      ],
      "metadata": {
        "id": "KzTpuDGaMQ6B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "X contains all the features, while y contains the target variable we aim to predict."
      ],
      "metadata": {
        "id": "s3NQ0QNyMZSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 9: Train-Test Split <a name=\"train-test-split\"></a>**\n",
        "Next, we will split our dataset into training and testing sets. This helps evaluate the performance of our models on unseen data."
      ],
      "metadata": {
        "id": "n36ukJEuMb-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "DNRLQvPCMX-N"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "train_test_split() splits the data into training (80%) and testing (20%) sets. The random_state parameter ensures reproducibility."
      ],
      "metadata": {
        "id": "99U1f6t9MgIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 10: Feature Scaling <a name=\"feature-scaling\"></a>**\n",
        "To standardize the feature values, we will apply StandardScaler. This step is essential as it ensures that all features contribute equally to the model's performance."
      ],
      "metadata": {
        "id": "iRb6WTj8Mh98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "FZ6nY6CTMeOm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "StandardScaler standardizes features by removing the mean and scaling to unit variance.\n",
        "We fit the scaler on the training data and then transform both the training and test sets to apply the same scaling."
      ],
      "metadata": {
        "id": "1ActdeSnMl3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 11: Display Preprocessed Data <a name=\"display-preprocessed-data\"></a>**\n",
        "Finally, we will display samples from the preprocessed training and testing datasets to verify our preprocessing steps."
      ],
      "metadata": {
        "id": "sTR16IIvMn-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the preprocessed data\n",
        "print(\"\\nPreprocessed X_train sample:\\n\", X_train_scaled[:5])\n",
        "print(\"Preprocessed X_test sample:\\n\", X_test_scaled[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1p5P-RzMkC7",
        "outputId": "34d61302-31fb-498e-b178-9a59db142120"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessed X_train sample:\n",
            " [[ 1.52718818  1.0901965   1.26183446  0.55282624 -1.31933367 -1.6966673\n",
            "   0.78895436 -0.46232073 -0.51176632]\n",
            " [-0.85051483 -0.92299623 -0.41351298 -0.99966729 -0.41318225 -1.6966673\n",
            "  -1.26750044 -0.46232073  1.95401684]\n",
            " [-0.85051483 -0.98134964 -0.95394763 -1.1247723   0.92792185  1.63897537\n",
            "  -1.26750044 -0.46232073  1.95401684]\n",
            " [-0.85051483 -0.98134964 -1.1701215  -1.39285445  0.27549283  0.52709448\n",
            "  -1.26750044 -0.46232073  1.95401684]\n",
            " [-0.85051483 -0.74793599 -0.22436085 -0.3276747  -0.23195197 -0.30681619\n",
            "  -1.26750044  2.16300056 -0.51176632]]\n",
            "Preprocessed X_test sample:\n",
            " [[-0.85051483 -0.98134964 -1.35927363 -1.39881183  0.63795339 -0.02884597\n",
            "  -1.26750044 -0.46232073  1.95401684]\n",
            " [-0.85051483 -0.69930815 -0.65670857 -0.40988656  1.07290607  1.63897537\n",
            "   0.78895436 -0.46232073 -0.51176632]\n",
            " [ 0.33833667  0.38995555 -0.08925218 -0.39916327 -0.9568731  -1.41869708\n",
            "   0.78895436 -0.46232073 -0.51176632]\n",
            " [ 1.52718818  1.22635446  1.26183446  1.15690469 -0.88438099 -0.02884597\n",
            "   0.78895436 -0.46232073 -0.51176632]\n",
            " [ 1.52718818  1.22635446  1.26183446  1.51077313 -0.41318225 -0.86275663\n",
            "   0.78895436 -0.46232073 -0.51176632]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "We print the first five samples of the scaled training and testing feature sets to inspect the results of our preprocessing."
      ],
      "metadata": {
        "id": "NULvvFjSMuEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of Data Preprocessing for Car Dataset\n",
        "\n",
        "In this notebook, we performed data preprocessing on a car dataset from the UCI Machine Learning Repository. The main steps involved are outlined below:\n",
        "\n",
        "## Steps Overview\n",
        "\n",
        "1. **Import Libraries**:\n",
        "   - We imported `pandas` for data manipulation and `sklearn` for data preprocessing tasks.\n",
        "\n",
        "2. **Load the Dataset**:\n",
        "   - The dataset was loaded from a specified URL into a pandas DataFrame.\n",
        "\n",
        "3. **Explore the Initial Data**:\n",
        "   - Checked the shape, data types, and missing values in the dataset to understand its structure.\n",
        "\n",
        "4. **Rename Columns**:\n",
        "   - Column names were renamed for easier reference and readability.\n",
        "\n",
        "5. **Handle Missing Values**:\n",
        "   - Missing values in the `horsepower` column were converted to numeric and replaced with the mean of the column.\n",
        "\n",
        "6. **Drop Unnecessary Columns**:\n",
        "   - The `car_name` column was removed as it did not contribute to our analysis.\n",
        "\n",
        "7. **One-Hot Encoding of Categorical Variables**:\n",
        "   - The `origin` column was one-hot encoded to convert it into a binary format suitable for modeling.\n",
        "\n",
        "8. **Separate Features and Target Variable**:\n",
        "   - The dataset was divided into features (`X`) and the target variable (`y`, MPG).\n",
        "\n",
        "9. **Train-Test Split**:\n",
        "   - The dataset was split into training and testing sets (80% train, 20% test) for model evaluation.\n",
        "\n",
        "10. **Feature Scaling**:\n",
        "    - Standardization was applied to the features using `StandardScaler` to ensure all features contribute equally to model performance.\n",
        "\n",
        "11. **Display Preprocessed Data**:\n",
        "    - Samples from the preprocessed training and testing datasets were printed to verify the results.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The dataset has been successfully preprocessed, making it ready for further analysis or machine learning modeling. This includes handling missing values, encoding categorical variables, and standardizing features.\n",
        "\n",
        "Feel free to expand on this analysis with machine learning models to predict MPG based on the processed features.\n"
      ],
      "metadata": {
        "id": "oi08KFcJM1Si"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MarnzKm9MspX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}